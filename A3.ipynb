{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python394jvsc74a57bd03f3a6e375f0cb6c8686eeb484d16b145563b57fcb38bc08fd5f1d35ccf3f9641",
      "display_name": "Python 3.9.4 64-bit ('ml': conda)"
    },
    "metadata": {
      "interpreter": {
        "hash": "3f3a6e375f0cb6c8686eeb484d16b145563b57fcb38bc08fd5f1d35ccf3f9641"
      }
    },
    "colab": {
      "name": "A3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEATqpzl1qSA"
      },
      "source": [
        "# Assignment 3 Report\n",
        "## Introduction\n",
        "The experiment aims to implement multinomial Naïve Bayesian algorithms from scratch, and try to extend the NB to get an algorithm with better performance.\n",
        "\n",
        "## Methodology\n",
        "### Coding\n",
        "The project is devided into three parts:\n",
        "1. Reading data,\n",
        "2. vectorize the abstracts,\n",
        "3. train and test model.\n",
        "\n",
        "#### 1. Reading data\n",
        "For this part, I created a `DataFrame` class, this class is designed and coded to simulate a `pan das.DataFrame`. Therefore the object of this class can be considered a dataframe, with some basic functions enabled. `train_test_split` is implemented as a instance object to split the training and testing set. \n",
        "#### 2. Count  Vectorizer\n",
        "I created a CountVectorizer class for this part. `CountVectorizer` is created to simulate some basic functions provided by `sklearn.feature_extraction.CountVectorizer`. Any instance of this class is a vectorizer, use `~.fit` to feed it with texts to generate a **bag of words**, and after that use `~.transform` to transform any abstracts into a **text vector**. \n",
        "#### 3. MultinomialNB\n",
        "I created a MultinomialNB class for this part. `MultinomialNB` is created to simulate some basic functions provided by `sklearn.naive_bayes.MultinomialNB`. Any instance of this class is a classifier. `~.fit` function takes train set and train the classifier by calculating parameters like **priors** and **likelihoods**, then store them as some instance variables, TF-IDF can be enabled with passing the boolean parameter `idf`. `~.predict` uses the parameters generated form `~.fit` to calculate the posterior probability. Due to technical issue, metric methods are integrated in MultinomialNB class. \n",
        "##### 3.1 IDF smoothing\n",
        "IDF originally is calculated by $$\\forall w\\in W, IDF = \\log(\\frac{N}{D_w})$$However in case there are no documents that have a certain word. I made it $$\\forall w\\in W, IDF = \\log(\\frac{N+1}{D_w+1})+1$$\n",
        "##### 3.2 Likelihood Smoothing\n",
        "likelihood originally is calculated by $$\\mathbb{P}(w_i|V) = \\frac{w_i}{\\sum_{x=0}^{x= |V| } x_v}$$However, in case some words have likelihoods of 0 which makes the posterior probability 0, I made it $$\\mathbb{P}(w_i|V) = \\frac{w_i+1}{\\sum_{x=0}^{x= |V| } x_v+|V| }$$\n",
        "\n",
        "### Experiment Designs\n",
        "The experiment is implemented in two conditions. In order to make the running time reasonable, 1000 features are selected. First I try the algorithm without TF-IDF, then I try the algorithm with TF-IDF. \n",
        "\n",
        "Cross validation is used to evaluate overall performance of the clasifier on whole dataset. With repeatly doing cross validation 16 times on randomly shuffled dataset to make sure **chance is not acting alone**.\n",
        "\n",
        "### Expected Outcomes\n",
        "Applying TF-IDF instead of simply using the frequency of each word reflects the importance of a word to a document in a bag of word. For any word, The more documents that this word is in, the lower IDF it is. The less documents this word is in, the higher IDF it is. \n",
        "\n",
        "## Results and Findings\n",
        "Let $H_0$be the mean accuracy of two situations are same.\n",
        "\n",
        "Let $H_A$be the mean accuracy of two situations are different.\n",
        "\n",
        "The repeated cross_val sections shows the sample mean accuracy of the classifier without tf-idf is 0.8859531250000001, and the $\\sigma$\tis 0.001247556204896173, and those of the classifier with tf-idf are 0.8859531250000001\tand 0.0012475562048961737. \n",
        "\n",
        "The sample size of both samples are 16, therefore degree of freedom is 30. \n",
        "\n",
        "With two samples t-test, we can see the p-value = 0.016. This rejects our null hypothesis that the mean accuracy with no TF-IDF is same as that of using TF-IDF. With the mean accuracies returned from the repeated cross_val. With 95% confidence, we can say, the mean accuracy of using TF-IDF is significantly higher than that of not using TF-IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNF3Rcgw4BnC"
      },
      "source": [
        "# Code\n",
        "## Initializations, Class Definitions and Type Declarations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdsOPft51qSD"
      },
      "source": [
        "# from DataFrame import DataFrame\n",
        "# from CountVectorizer import CountVectorizer\n",
        "# from MultinomialNB import MultinomialNB,get_accuracy, cross_val_score\n",
        "import numpy as np\n",
        "from math import log, exp, sqrt, pow\n",
        "\n",
        "rs = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5ZA_Z-f1qSE",
        "outputId": "0b3de7a6-3ae9-48d3-f26e-e4009c4e0368"
      },
      "source": [
        "#  Copyright (c) 2021 Jäger. All rights reserved.\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class CountVectorizer:\n",
        "\n",
        "    def __init__(self, stop_words: List[str], max_features: int = None):\n",
        "        self.vocabularies = []\n",
        "        self.stop_words = stop_words\n",
        "        self.max_features = max_features\n",
        "        pass\n",
        "\n",
        "    def fit(self, raw_documents) -> None:\n",
        "        raw_documents_all_in_1 = ' '.join(raw_documents)\n",
        "        all_words = np.array(self.text_process(raw_documents_all_in_1), dtype=object)\n",
        "        unique, counts = np.unique(all_words, return_counts=True)\n",
        "        bag_of_words = np.transpose(np.array([unique, counts], dtype=object))\n",
        "        self.vocabularies = bag_of_words[bag_of_words[:, 1].argsort()]  # sort\n",
        "        self.vocabularies = self.vocabularies[::-1]  # descending order\n",
        "        if self.max_features is not None:\n",
        "            self.vocabularies = self.vocabularies[:self.max_features]\n",
        "\n",
        "    def transform(self, raw_documents, tf_idf=False):\n",
        "        vectorized_document = np.empty(shape=(len(raw_documents), len(self.vocabularies)), dtype=object)\n",
        "        for i in range(len(raw_documents)):\n",
        "            words = self.text_process(raw_documents[i])\n",
        "            for j in range(len(self.vocabularies)):\n",
        "                vectorized_document[i, j] = words.count(self.vocabularies[:, 0][j])\n",
        "        if tf_idf:\n",
        "            return vectorized_document, CountVectorizer.idf(vectorized_document)\n",
        "        else:\n",
        "            return vectorized_document\n",
        "\n",
        "    def text_process(self, abstract: str) -> List[str]:\n",
        "        return [word.lower() for word in ''.join([char for char in abstract if char not in string.punctuation]).split()\n",
        "                if\n",
        "                word not in self.stop_words]\n",
        "\n",
        "    # noinspection PyPep8Naming\n",
        "    @staticmethod\n",
        "    def idf(vectorized_document: np.ndarray) -> np.ndarray:\n",
        "        N = len(vectorized_document)\n",
        "        D = [len(vectorized_document[vectorized_document[:, j] != 0]) for j in range(vectorized_document.shape[1])]\n",
        "        D = np.array(D, dtype=np.float)\n",
        "        IDF = np.log(N+1 / D+1)+1\n",
        "        return IDF\n",
        "#  Copyright (c) 2021 Jäger. All rights reserved.\n",
        "\n",
        "from typing import List\n",
        "from numpy import str, random, array, arange, genfromtxt, ndarray, asarray, empty\n",
        "\n",
        "\n",
        "class DataFrame:\n",
        "    col_names: ndarray\n",
        "    data: ndarray\n",
        "\n",
        "    def __init__(self, data: ndarray = None, col_names: ndarray = None):\n",
        "        self.col_names: ndarray = empty(0)\n",
        "        self.data: ndarray = empty((0, 0))\n",
        "        if data is not None:\n",
        "            self.data = data\n",
        "            if col_names is not None:\n",
        "                if len(col_names) != len(self.data):\n",
        "                    raise Exception('Columns names length mismatch')\n",
        "                else:\n",
        "                    self.col_names = asarray(col_names)\n",
        "                    pass\n",
        "                pass\n",
        "            pass\n",
        "        pass\n",
        "\n",
        "    def get_col(self, col: int) -> ndarray:\n",
        "        \"\"\"\n",
        "        Use index to get a specific column.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        col: int\n",
        "            Position index of the column.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray\n",
        "            The column at this position.\n",
        "        \"\"\"\n",
        "        return self.data[:, col]\n",
        "\n",
        "    def get_row(self, row: int) -> ndarray:\n",
        "        \"\"\"\n",
        "        Use index to get a specific row.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        row: int\n",
        "            Position index of the row.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray\n",
        "            The row at this position.\n",
        "        \"\"\"\n",
        "        return self.data[row]\n",
        "\n",
        "    def get_cols(self, cols: List[int]) -> ndarray:\n",
        "        \"\"\"\n",
        "        Use a list of indices to get a new DataFrame consists of specific columns.\n",
        "        Parameters\n",
        "        ----------\n",
        "        cols: List[int]\n",
        "            Position indices of the columns.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray\n",
        "            A DataFrame with these columns.\n",
        "        \"\"\"\n",
        "        return self.data[:, cols]\n",
        "\n",
        "    def get_rows(self, rows: List[int]) -> ndarray:\n",
        "        \"\"\"\n",
        "        Use a list of indices to get a new DataFrame consists of specific rows.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rows: List[int]\n",
        "            Position indices of the rows.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray\n",
        "            A DataFrame with these rows.\n",
        "        \"\"\"\n",
        "        return self.data[rows]\n",
        "\n",
        "    def get_sub_df(self, rows: List[int], cols: List[int]) -> ndarray:\n",
        "        \"\"\"\n",
        "        Use two lists of indices to get a new DataFrame consists of selected rows and columns.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        rows: List[int]\n",
        "            Position indices of the rows.\n",
        "        cols: List[int]\n",
        "            Position indices of the cols.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray\n",
        "            A sub-DataFrame with selected rows and columns by position indices.\n",
        "        \"\"\"\n",
        "        return self.data[array(rows)[:, None], array(cols)]\n",
        "\n",
        "    def read_csv(self, filename: str, sep: str = ',', col_names: List[str] = None):\n",
        "        \"\"\"\n",
        "        Read data from a .CSV file and save data to the caller object.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        filename:str\n",
        "            The path of the csv to be read.\n",
        "        sep:str, default = ','\n",
        "            Delimiter to use.\n",
        "        col_names:List[str], optional\n",
        "        \"\"\"\n",
        "        self.col_names = genfromtxt(filename, delimiter=sep, dtype=str, encoding='utf-8')[0]\n",
        "        self.data = genfromtxt(filename, delimiter=sep, dtype=str, encoding='utf-8')[1:]\n",
        "        if col_names is not None:\n",
        "            if len(col_names) != len(self.data):\n",
        "                raise Exception('Columns names length mismatch')\n",
        "            else:\n",
        "                self.col_names = asarray(col_names)\n",
        "                pass\n",
        "            pass\n",
        "        pass\n",
        "\n",
        "    def head(self, n: int = 5) -> ndarray:\n",
        "        \"\"\"\n",
        "        Return the first `n` rows.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n:int, default = 5\n",
        "            A positive integer which specifies the number of rows to select.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The first `n` rows of the caller object\n",
        "        \"\"\"\n",
        "        return self.data[0:n]\n",
        "        pass\n",
        "\n",
        "    def shape(self):\n",
        "        \"\"\"\n",
        "        Return the shape of the caller object.\n",
        "        Returns\n",
        "        -------\n",
        "        tuple[int]\n",
        "            A tuple with lengths of each axis of the caller object.\n",
        "        \"\"\"\n",
        "        return self.data.shape\n",
        "\n",
        "    def train_test_split(\n",
        "            self,\n",
        "            test_size: float = 0.2,\n",
        "            random_state: int = None,\n",
        "            shuffle: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Shuffle and split the dataframe.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        shuffle : bool, default = True\n",
        "            Specify whether or not shuffle before splitting.\n",
        "        test_size : float, default = 0.2\n",
        "            The size of the testing set after splitting.\n",
        "        random_state : int, optional\n",
        "            Specify to reproduce same result when Running many times.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple[DataFrame, DataFrame]\n",
        "            The split training set and testing set.\n",
        "        \"\"\"\n",
        "        index_list = arange(len(self.data))\n",
        "        if shuffle:\n",
        "            if random_state is not None:\n",
        "                random.default_rng(random_state).shuffle(index_list)\n",
        "            else:\n",
        "                random.default_rng().shuffle(index_list)\n",
        "\n",
        "        border = int(len(index_list) * (1 - test_size))\n",
        "        train_indices = index_list[:border]\n",
        "        test_indices = index_list[border:]\n",
        "\n",
        "        return DataFrame(self.get_rows([train_indices])), DataFrame(self.get_rows([test_indices]))\n",
        "#  Copyright (c) 2021 Jäger. All rights reserved.\n",
        "\n",
        "import numpy as np\n",
        "from math import log, pow, exp\n",
        "\n",
        "\n",
        "class MultinomialNB:\n",
        "    def __init__(self) -> None:\n",
        "        self.y_labels = []\n",
        "        self.grouped_Xs = []\n",
        "        self.class_priors = []\n",
        "        self.likelihoods_by_class = []\n",
        "        pass\n",
        "\n",
        "    def fit(self, vec_X, y, idf=None):\n",
        "        self.y_labels = np.unique(y).tolist()\n",
        "        self.grouped_Xs = [vec_X[y == self.y_labels[i]] for i in range(len(self.y_labels))]\n",
        "        self.class_priors = [vec_x.shape[0] / vec_X.shape[0] for vec_x in self.grouped_Xs]\n",
        "        if idf is None:\n",
        "            self.likelihoods_by_class = [(vec_x.sum(axis=0) + 1) / (vec_x.sum() + vec_X.shape[1]) for vec_x in\n",
        "                                         self.grouped_Xs]\n",
        "        else:\n",
        "            self.likelihoods_by_class = [(vec_x.sum(axis=0) * idf + 1) / (sum(vec_x.sum(axis=0) * idf) + vec_X.shape[1])\n",
        "                                         for vec_x in self.grouped_Xs]\n",
        "\n",
        "    def predict(self, vectorized_samples):\n",
        "        predictions = []\n",
        "        for sample in vectorized_samples:\n",
        "            temp = []  # list of 4 posterior_pr\n",
        "            for c in range(len(self.grouped_Xs)):\n",
        "                sample_likelihood = 1.0\n",
        "                # log_sample_likelihood = log(sample_likelihood)\n",
        "                for i in range(len(sample)):  # for word in words\n",
        "                    for j in range(sample[i]):\n",
        "                        # log_sample_likelihood = log_sample_likelihood + log(self.likelihoods_by_class[c][i])\n",
        "                        sample_likelihood = sample_likelihood * self.likelihoods_by_class[c][i]\n",
        "                # temp.append(exp(sample_likelihood + log(self.class_priors[c])))\n",
        "                temp.append(sample_likelihood*self.class_priors[c])\n",
        "            # print(temp)\n",
        "            predictions.append(self.y_labels[temp.index(np.max(temp))])\n",
        "        return predictions\n",
        "\n",
        "def get_accuracy(estimates, true_val, verbose = False):\n",
        "    n = 0\n",
        "    for pair in np.array([estimates, true_val]).transpose():\n",
        "        if pair[0] == pair[1]:\n",
        "            n += 1\n",
        "    if verbose:\n",
        "        print(f\"Correct: {n}\\tAccuracy: {n / len(estimates):.4%}\")\n",
        "    return n / len(estimates)\n",
        "\n",
        "\n",
        "def cross_val_score(X: np.ndarray, y: np.ndarray, idf=None, cv: int = 5):\n",
        "    index_list = np.arange(X.shape[0])\n",
        "    indices_folds = np.asarray(np.array_split(index_list, cv))\n",
        "    results = np.zeros(cv)\n",
        "    for i in range(cv):\n",
        "        val_fold_indices = indices_folds[i]\n",
        "        # np.delete(indices_folds, i)\n",
        "        train_fold_indices = indices_folds[[j for j in range(cv) if j!=i]].flatten()\n",
        "        X_val = X[val_fold_indices]\n",
        "        y_val = y[val_fold_indices]\n",
        "        X_train = X[train_fold_indices]\n",
        "        y_train = y[train_fold_indices]\n",
        "        model = MultinomialNB()\n",
        "        if idf is not None:\n",
        "            model.fit(X_train, y_train, idf)\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_val)\n",
        "        results[i] = get_accuracy(predictions, y_val)\n",
        "    return results\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-2-9ff57ad9f583>:53: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from numpy import str, random, array, arange, genfromtxt, ndarray, asarray, empty\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEbSNqMQ4ZBM"
      },
      "source": [
        "## Read Data\n",
        "Here The data file is expected to be at the root directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeLsYiqH1qSF"
      },
      "source": [
        "df = DataFrame()\n",
        "df.read_csv('trg.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfduufar1qSG",
        "outputId": "c9930535-1ec1-4c2c-cd2a-22ed1c88f855"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['1', 'B',\n",
              "        '\"the 4 202 353 bp genome of the alkaliphilic bacterium bacillus halodurans c-125 contains 4066 predicted protein coding sequences cdss 2141 527 of which have functional assignments 1182 29 of which are conserved cdss with unknown function and 743 18 3 of which have no match to any protein database among the total cdss 88 match sequences of proteins found only in bacillus subtilis and 667 are widely conserved in comparison with the proteins of various organisms including bsubtilis the b halodurans genome contains 112 transposase genes indicating that transposases have played an important evolutionary role in horizontal gene transfer and also in internal genetic rearrangement in the genome strain c-125 lacks some of the necessary genes for competence such as coms srfa and rapc supporting the fact that competence has not been demonstrated experimentally in c-125 there is no paralog of tupa encoding teichuronopeptide which contributes to alkaliphily in the c-125 genome and an ortholog of tupa cannot be found in the bsubtilis genome out of 11 sigma factors which belong to the extracytoplasmic function family 10 are unique to b halodurans suggesting that they may have a role in the special mechanism of adaptation to an alkaline environment\"'],\n",
              "       ['2', 'A',\n",
              "        '\"the complete 1751377-bp sequence of the genome of the thermophilic archaeon methanobacterium thermoautotrophicum deltah has been determined by a whole-genome shotgun sequencing approach a total of 1855 open reading frames orfs have been identified that appear to encode polypeptides 844 46 of which have been assigned putative functions based on their similarities to database sequences with assigned functions a total of 514 28 of the orf-encoded polypeptides are related to sequences with unknown functions and 496 27 have little or no homology to sequences in public databases comparisons with eucarya- bacteria- and archaea-specific databases reveal that 1013 of the putative gene products 54 are most similar to polypeptide sequences described previously for other organisms in the domain archaea comparisons with the methanococcus jannaschii genome data underline the extensive divergence that has occurred between these two methanogens only 352 19 of m thermoautotrophicum orfs encode sequences that are 50 identical to m jannaschii polypeptides and there is little conservation in the relative locations of orthologous genes when the m thermoautotrophicum orfs are compared to sequences from only the eucaryal and bacterial domains 786 42 are more similar to bacterial sequences and 241 13 are more similar to eucaryal sequences the bacterial domain-like gene products include the majority of those predicted to be involved in cofactor and small molecule biosyntheses intermediary metabolism transport nitrogen fixation regulatory functions and interactions with the environment most proteins predicted to be involved in dna metabolism transcription and translation are more similar to eucaryal sequences gene structure and organization have features that are typical of the bacteria including genes that encode polypeptides closely related to eucaryal proteins there are 24 polypeptides that could form two-component sensor kinase-response regulator systems and homologs of the bacterial hsp70-response proteins dnak and dnaj which are notably absent in m jannaschii dna replication initiation and chromosome packaging in m thermoautotrophicum are predicted to have eucaryal features based on the presence of two cdc6 homologs and three histones however the presence of an ftsz gene indicates a bacterial type of cell division initiation the dna polymerases include an x-family repair type and an unusual archaeal b type formed by two separate polypeptides the dna-dependent rna polymerase rnap subunits a\\' a b\\' b and h are encoded in a typical archaeal rnap operon although a second a\\' subunit-encoding gene is present at a remote location there are two rrna operons and 39 trna genes are dispersed around the genome although most of these occur in clusters three of the trna genes have introns including the trnapro ggg gene which contains a second intron at an unprecedented location there is no selenocysteinyl-trna gene nor evidence for classically organized is elements prophages or plasmids the genome contains one intein and two extended repeats 36 and 86 kb that are members of a family with 18 representatives in the m jannaschii genome\"'],\n",
              "       ['3', 'E',\n",
              "        '\"in 1992 we started assembling an ordered library of cosmid clones from chromosome xiv of the yeast saccharomyces cerevisiae at that time only 49 genes were known to be located on this chromosome and we estimated that 80 to 90 of its genes were yet to be discovered in 1993 a team of 20 european laboratories began the systematic sequence analysis of chromosome xiv the completed and intensively checked final sequence of 784328 base pairs was released in april 1996 substantial parts had been published before or had previously been made available on request the sequence contained 419 known or presumptive protein-coding genes including two pseudogenes and three retrotransposons 14 trna genes and three small nuclear rna genes for 116 30 protein-coding sequences one or more structural homologues were identified elsewhere in the yeast genome half of them belong to duplicated groups of 6-14 loosely linked genes in most cases with conserved gene order and orientation relaxed interchromosomal synteny we have considered the possible evolutionary origins of this unexpected feature of yeast genome organization\"'],\n",
              "       ['4', 'E',\n",
              "        '\"the aim of this study is to measure human mitochondrial sequence variability in the relatively slowly evolving mitochondrial gene cytochrome oxidase subunit ii coii and to estimate when the human common ancestral mitochondrial type existed new coii gene sequences were determined for five humans homo sapiens including some of the most mitochondrially divergent humans known for two pygmy chimpanzees pan paniscus and for a common chimpanzee p troglodytes coii sequences were analyzed with those from another relatively slowly evolving mitochondrial region nd4-5 from class 1 third codon position sequence data a relative divergence date for the human mitochondrial ancestor is estimated as 127 th of the human-chimpanzee divergence time if it is assumed that humans and chimpanzees diverged 6 mya this places a human mitochondrial ancestor at 222000 years significantly different from 1 myr the presumed time of an h erectus emergence from africa the mean coalescent time estimated from all 1580 sites of combined mitochondrial data when a 6-mya human-chimpanzee divergence is assumed is 298000 years with 95 confidence interval of 129000-536000 years neither estimate is compatible with a 1-myr-old human mitochondrial ancestor the mitochondrial dna sequence data from coii and nd4-5 regions therefore do not support this multiregional hypothesis for the emergence of modern humans\"'],\n",
              "       ['5', 'B',\n",
              "        '\"the amino acid sequence of the spirulina maxima ferredoxin was shown to be h2n-ala-thr-tyr-lys-val-thr-leu-ile-ser-glu-ala-glu-gly-ile-asn-glu-thr-ile-asp-cys-asp-asp-asp-thr-tyr-ile-leu-asp-ala-ala-glu-glu-ala-gly-leu-asp-leu-pro-tyr-ser-cys-arg-ala-gly-ala-cys-ser-thr-cys-ala-gly-lys-ile-thr-ser-gly-ser-ile-asp-gln-ser-asp-gln-ser-phe-leu-asp-asp-asp-gln-ile-gln-ala-gly-tyr-val-leu-thr-cys-val-ala-tyr-pro-thr-ser-asp-cys-thr-ile-gln-thr-his-gln-glu-glu-gly-leu-tyr-cooh the s maxima ferredoxin is the first procaryote ferredoxin of the plant-algal type to be reported a modification of the automated sequence determination of a peptide which was extracted by the organic solvents used to remove excess reagents and the amino acid thiazoline was utilized to complete the sequence of a 36 residue tryptic peptide\"']],\n",
              "      dtype='<U3157')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9gCQlaM1qSH"
      },
      "source": [
        "## Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t25yKkQ31qSH",
        "outputId": "230a5799-19f8-4aec-e99e-1092e2ea89f6"
      },
      "source": [
        "X_raw = df.get_col(2)\n",
        "y = df.get_col(1)\n",
        "train, val = df.train_test_split()\n",
        "X_train_raw = train.get_col(2)\n",
        "X_val_raw = val.get_col(2)\n",
        "y_train = train.get_col(1)\n",
        "y_val = val.get_col(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-2-9ff57ad9f583>:136: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
            "  return self.data[rows]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOwBvTVX1qSH"
      },
      "source": [
        "## Vectorization\n",
        "\n",
        "Vectorization uses a list of stopwords. The stopwords should be a txt file at the root directory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihqGlQKU1qSI"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words=np.genfromtxt('stopwords.txt', dtype = str), max_features=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zuIOW-N1qSI",
        "outputId": "2ce6c6b3-29b8-4957-9d90-cf9c5a287a3f"
      },
      "source": [
        "vectorizer.fit(X_raw)\n",
        "vec_X_train, idf_train = vectorizer.transform(X_train_raw, tf_idf=True)\n",
        "vec_X, idf = vectorizer.transform(X_raw, tf_idf=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-2-9ff57ad9f583>:47: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  D = np.array(D, dtype=np.float)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWbdzXFt1qSI",
        "outputId": "9326b8c6-28b0-473a-eb96-f4709b51b226"
      },
      "source": [
        "vec_X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [3, 0, 1, ..., 0, 0, 0],\n",
              "       [0, 3, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 2, 2, ..., 0, 0, 0],\n",
              "       [0, 1, 6, ..., 0, 0, 0],\n",
              "       [12, 3, 2, ..., 0, 0, 0]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAE5lil-1qSJ"
      },
      "source": [
        "## MultinomialNB "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBdXgmac1qSJ"
      },
      "source": [
        "### Max Features = 1000, no TF_IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQyUNjfP1qSJ"
      },
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(vec_X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "Ikk_IvTb1qSJ"
      },
      "source": [
        "predictions_on_train = model.predict(vec_X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U9wu5HG1qSJ",
        "outputId": "df5b4264-44f5-4b2f-fc77-9d3fbfa3fd6b"
      },
      "source": [
        "get_accuracy(predictions_on_train, y_train, verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct: 2833\tAccuracy: 88.5312%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8853125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPkhycbB1qSK"
      },
      "source": [
        "vec_X_val = vectorizer.transform(X_val_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "qUr0RJHa1qSK"
      },
      "source": [
        "predictions_on_val = model.predict(vec_X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjY_xfqM1qSK",
        "outputId": "e217ec27-4b4f-45cb-8e0c-8d7adc30b66c"
      },
      "source": [
        "get_accuracy(predictions_on_val, y_val, verbose = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct: 692\tAccuracy: 86.5000%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.865"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D53bMu51qSL"
      },
      "source": [
        "#### Repeated Cross Validation\n",
        "16 times repeated cross validation, dataset is shuffled randomly each time before cross validation. See `MultinomialNB` class for cross validation method implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H1BKrCN1qSL",
        "outputId": "828fc5de-d4d4-4258-f37b-19335befd260"
      },
      "source": [
        "rep = 16\n",
        "results = np.zeros(rep)\n",
        "for i in range(rep):\n",
        "    index_list = np.arange(len(df.data))\n",
        "    np.random.default_rng(rs*(i+1)).shuffle(index_list)\n",
        "    vec_X_shuffled = vec_X[index_list]\n",
        "    vec_Y_shuffled = y[index_list]\n",
        "    results[i]=np.mean(cross_val_score(vec_X_shuffled, vec_Y_shuffled))\n",
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.87425, 0.873  , 0.87475, 0.87025, 0.872  , 0.87325, 0.872  ,\n",
              "       0.8685 , 0.87175, 0.8705 , 0.87275, 0.8735 , 0.87   , 0.87175,\n",
              "       0.867  , 0.87225])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhPbL5UL1qSL",
        "outputId": "e9d3e5fd-e703-4dc2-ae59-7a8666071174"
      },
      "source": [
        "print(f'mean: {np.mean(results)}\\tstd: {np.std(results)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean: 0.87171875\tstd: 0.0019919270663103566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Rr494f1qSL"
      },
      "source": [
        "### Use TF_IDF, Max Features = 1000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWLJgbP91qSM"
      },
      "source": [
        "model_tf_idf = MultinomialNB()\n",
        "model_tf_idf.fit(vec_X_train, y_train, idf = idf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnLwly4k1qSM"
      },
      "source": [
        "predictions_tf_idf = model_tf_idf.predict(vec_X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOfpBPIJ1qSM",
        "outputId": "976c6dda-f16b-432e-e030-5b430f8391bc"
      },
      "source": [
        "get_accuracy(predictions_tf_idf, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.89625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGn62go-5RWg"
      },
      "source": [
        "#### Repeated Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2V0NBG41qSM",
        "outputId": "273bb43f-8b1b-4ab1-cd12-890c6ea5f8d5"
      },
      "source": [
        "rep = 16\n",
        "results_tf_idf = np.zeros(rep)\n",
        "for i in range(rep):\n",
        "    index_list = np.arange(len(df.data))\n",
        "    np.random.default_rng().shuffle(index_list)\n",
        "    vec_X_shuffled = vec_X[index_list]\n",
        "    vec_Y_shuffled = y[index_list]\n",
        "    results_tf_idf[i]=np.mean(cross_val_score(vec_X_shuffled, vec_Y_shuffled, idf))\n",
        "results_tf_idf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8875 , 0.8845 , 0.8845 , 0.88575, 0.884  , 0.887  , 0.88725,\n",
              "       0.88675, 0.885  , 0.88675, 0.8845 , 0.885  , 0.88725, 0.88625,\n",
              "       0.888  , 0.88525])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svc1quJB1qSN",
        "outputId": "77ea7f0f-6c6f-4fe1-9a19-f664a0f22e0f"
      },
      "source": [
        "print(f'mean: {np.mean(results_tf_idf)}\\tstd: {np.std(results_tf_idf)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean: 0.8859531250000001\tstd: 0.0012475562048961737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBvTAOlW1qSN"
      },
      "source": [
        "## Use scipy to do hypotheses testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6V-ac68y1qSN"
      },
      "source": [
        "from scipy.stats import sem, t\n",
        "mean1 = 0.87171875\t\n",
        "mean2 = 0.8732656249999999\t\n",
        "std1=0.0019919270663103566\n",
        "std2=0.0008075268474639141\n",
        "\n",
        "n1, n2 = 16, 16\n",
        "se1, se2 = std1/sqrt(n1), std2/sqrt(n2)\n",
        "\n",
        "se1, se2 = sem(results), sem(results_tf_idf)\n",
        "\n",
        "sed = sqrt(se1**2.0 + se2**2.0)\n",
        "\n",
        "t_stat = (mean1 - mean2) / sed\n",
        "df = 30\n",
        "\n",
        "alpha = 0.05\n",
        "cv = t.ppf(1.0 - alpha, df)\n",
        "\n",
        "p = (1 - t.cdf(abs(t_stat), df)) * 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYxY90LO5WBz"
      },
      "source": [
        "P value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esPIPxmP1qSN",
        "outputId": "c8882e21-0863-4455-a00d-04b48b242f75"
      },
      "source": [
        "p"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.016157454136917826"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}